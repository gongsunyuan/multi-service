import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.nn import DataParallel
from torch_geometric.loader import DataListLoader
from tqdm import tqdm
import torch.multiprocessing

from .DijkstraGnn import GNNPretrainModel, GLOBAL_STATS, DynamicGraphDataset
from ...Env.NetworkGenerator import TopologyGenerator

if __name__ == "__main__":
  try:
    torch.multiprocessing.set_start_method('spawn', force=True)
  except RuntimeError:
    pass
  try:
    torch.multiprocessing.set_sharing_strategy('file_system')
  except RuntimeError:
    pass

  print("ğŸš€ å¼€å§‹é˜¶æ®µ 1B: GNN ä¸»ä½“é¢„è®­ç»ƒ (æœ€ç»ˆå†²åˆºç‰ˆ)...")

  # --- 1. è¶…å‚æ•°é…ç½® ---
  EPOCHS = 200          # [å†²åˆºä¼˜åŒ–] å¢åŠ åˆ° 150 è½®ï¼Œç»™ä½ LR æ›´å¤šæ—¶é—´
  GNN_DIM = 256         # [å†²åˆºä¼˜åŒ–] å®½åº¦ç¿»å€ï¼š128 -> 256
  NUM_LAYERS = 6
  BATCH_SIZE = 128       
  SAMPLES_PER_EPOCH = 6400
  LEARNING_RATE = 1e-3
  NODE_FEAT_DIM = 5
  EDGE_FEAT_DIM = 2

  if torch.cuda.is_available():
      torch.cuda.init()
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  print(f"Using device: {device}")

  # --- 2. åˆå§‹åŒ–ç»„ä»¶ ---
  topo_gen = TopologyGenerator(num_nodes_range=(20, 30), m_ba=2)
  model = GNNPretrainModel(NODE_FEAT_DIM, GNN_DIM, EDGE_FEAT_DIM, NUM_LAYERS)
  model = model.to(device)

  if torch.cuda.device_count() > 1:
      print(f"âœ¨ å¯ç”¨ {torch.cuda.device_count()} å¼  GPU è¿›è¡Œ PyG DataParallel åŠ é€Ÿ")
      model = DataParallel(model)
      
  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

  from torch.optim import lr_scheduler
  # [å¾®è°ƒ] patience ä» 5 å¢åŠ åˆ° 8ï¼Œè®©å®ƒåœ¨é™ LR å‰å¤šå°è¯•ä¸€ä¼šå„¿
  scheduler = lr_scheduler.ReduceLROnPlateau(
      optimizer, mode='min', factor=0.1, patience=8, threshold=0.001, min_lr=1e-6
  )

  POS_WEIGHT_FIXED = torch.tensor([15.0]).to(device)
  loss_fn = nn.BCEWithLogitsLoss(pos_weight=POS_WEIGHT_FIXED)

  # --- 3. è®­ç»ƒå¾ªç¯ ---
  for epoch in range(EPOCHS):
    model.train()
    dataset = DynamicGraphDataset(topo_gen, GLOBAL_STATS, max_samples=SAMPLES_PER_EPOCH)
    train_loader = DataListLoader(dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)

    total_loss = 0.0
    total_acc = 0.0
    num_batches = 0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}", unit="batch")

    for batch_data_list in pbar:
      optimizer.zero_grad()
      edge_logits = model(batch_data_list)
      y_true = torch.cat([data.y for data in batch_data_list]).to(device)
      loss = loss_fn(edge_logits, y_true)
      loss.backward()
      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
      optimizer.step()

      current_loss = loss.item()
      total_loss += current_loss
      predicted = (edge_logits > 0.0).float()
      current_acc = (predicted == y_true).float().mean().item()
      total_acc += current_acc
      num_batches += 1
      pbar.set_postfix({"Loss": f"{current_loss:.4f}", "Acc": f"{current_acc:.2%}"})

    avg_loss = total_loss / num_batches
    avg_acc = total_acc / num_batches
    current_lr = optimizer.param_groups[0]['lr']
    print(f"Epoch {epoch+1} å®Œæˆ. Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.2%}, LR: {current_lr:.2e}")

    scheduler.step(avg_loss)

    if (epoch + 1) % 10 == 0:
      model_to_save = model.module if isinstance(model, DataParallel) else model
      torch.save(model_to_save.state_dict(), f'./MS/GNN/Pretrain/gnn_pretrained_epoch_{epoch+1}.pth')

  print("âœ… é˜¶æ®µ 1B é¢„è®­ç»ƒå®Œæˆï¼")
  model_to_save = model.module if isinstance(model, DataParallel) else model
  torch.save(model_to_save.state_dict(), './MS/GNN/pretrained_model.pth')